{"cells":[{"cell_type":"markdown","metadata":{},"source":["# NLP - Toxic Comments Classifier (Report)\n","\n","Team Number: 6\n","<br/>\n","Team Members: Dina Boshnaq, Iris Loret, Ingrid Hansen\n","\n","## Introduction\n","For this assignment we are doing classification of comments to check if they're toxic or not. The dataset used is the Toxic Comment Classification dataset from Kaggle. We will be doing single label classification (toxic or not toxic) instead of multi label classification. We are using a Transformer model from Hugging Face, specifically the DistilBERT base model (uncased). We are doing transfer learning by using the pre-trained tokenizer from the language model (DistilBERT) to initialize a new tokenizer and then build our own model based on it.\n","We split our dataset then train the model, and finally make predictions on text that we pass to the model.\n","\n","We are running our code on Kaggle since it has a closer connection to the data source so it uses less memory. Initially we had tried Colab but we were using too much memory so we switched to Kaggle."]},{"cell_type":"markdown","metadata":{},"source":["## EDA\n","We start off by downloading the dataset from Kaggle, we will be using the train.csv file to train our data. We load the data file in a dataset on Kaggle. Then we load that file as a pandas dataframe in our code.\n","\n","We did a simple EDA on the data where we checked the different columns and their datatypes.\n","\n","<img src=\"reportImages/info.jpg\" width=\"400\" style=\"border:solid 1px white;\"/>\n","\n","We have a comment_text column with the comments, and 6 different types of toxic comments. But since we're only doing single label classification, we want to have one column as the label to predict. We make one column called is_toxic. That's why for each row, we check if any of the 6 types of toxic comments are set to 1 (1 meaning it is of that type of toxicity, 0 meaning it's not). If at least one of them is set to 1, we set the value in is_toxic to 1. If none of them are valued 1, we set it to 0. Then we drop the 6 columns, keeping only comment_text and is_toxic. We also dropped the id column since there's no use for it.\n","\n","When checking the value counts of the is_toxic column, the 0 value had 143346 entries while the 1 value had 16225. This causes an imbalance in the data, and it's also too much data since we were still testing it. Therefore, we undersampled and balanced the data out by taking only 2000 samples from each (the 0 entries, and the 1 entries). This will make training the model faster and easier. If we can get this to run then we know it's working and we can increase the number of samples to make a better model.\n","\n","An issue that was encountered later on when trying to train the model is that it wasn't seeing the is_toxic column as the target label column, so we had to rename it to \"label\" per the documentation.\n","\n","Up to this point, the data type of label is integer and it's either 0 or 1. But this isn't enough to identify the labels, especially since we will be assigning the number of labels to 2 when making the model. The model configurations, especially the num_labels parameter, need to match the number of unique labels used during training. id2label and label2id should be correctly configured in the config.json file in the model directory.\n","If the original labels are numeric (0 and 1), the model might expect a mapping like id2label = {0: '0', 1: '1'} and label2id = {'0': 0, '1': 1}. However, if we use string labels, the mapping would be more intuitive: id2label = {'Not Toxic': 0, 'Toxic': 1} and label2id = {0: 'Not Toxic', 1: 'Toxic'}. We tried both and the second approach worked where we first map 0 to \"Not Toxic\" and 1 to \"Toxic\", then encode the labels:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["id2label = {0: \"Not Toxic\", 1: \"Toxic\"}\n","label2id = {\"Not Toxic\": 0, \"Toxic\": 1}\n","df_toxic_balanced[\"label\"] = df_toxic_balanced[\"label\"].apply(lambda x: label2id[x])"]},{"cell_type":"markdown","metadata":{},"source":["This will result in the configuration in the config.json (which will be produced later on when making the model) to be correct later on where we'd have the 2 labels and the correct problem type \"single label classification\".\n","\n","The final correct config.json:\n","\n","<img src=\"reportImages/rightConfig.jpg\" width=\"400\" style=\"border:solid 1px white;\"/>\n","\n","The initial wrong config.json:\n","\n","<img src=\"reportImages/wrongConfig.jpg\" width=\"400\"/>\n","\n","\n","We then make the hugging face dataset from our dataframe in order to apply tokenization on it."]},{"cell_type":"markdown","metadata":{},"source":["## Tokenizing the data\n","\n","The pre-trained transformer model we're using is a distilled version of the BERT base model (DistilBERT), specifically the uncased one (case insensitive).\n","We decided to try this one first since it's smaller and faster than BERT but based on it. It is self-supervised and uses Masked language modeling (MLM) which is in a way good since it can learn in a bidirectional representation of the sentence. It should get the job done, but of course, there are other transformer models out there that are more accurate in classifying comments based on toxicity, however they're bigger and will take a longer time in training. Since we're testing things out first, this model suffices.\n","\n","A tokenizer is needed in order to convert raw input (sentences) into smaller units such as words, this will help the ML models to understand and process the input. \n","We initialize the tokenizer to be used using the AutoTokenizer class from Hugging face transformers library. This class will help us load a pre-trained tokenizer for our model from the pre-trained DistilBERT Uncased model. We set the use_fast parameter to True, this will enable the use of a fast tokenizer.\n","\n","A function named \"preprocess\" is created to map the tokenizer to the dataset, specifically to the column \"comment_text\" which contains the input text to be analyzed. It then applies the tokenizer on each input sentence. The tokenizer is configured with the parameters truncation=True and max_length=128, indicating that it should truncate sequences longer than 128 tokens while ensuring that sequences are not longer than this maximum length. This will ensure that the input sequences have a consistent length which makes the processing by the model more efficient. So in summary, through this function we will prepare the dataset for further processing by transforming the raw text in the \"comment_text\" column into tokenized sequences suitable for input to natural language processing models.\n","\n","We apply the preprocess function on our dataset. The mapping function will be applied in batches rather than individually for each sample (batched=True). Then, a DataCollatorWithPadding object (DataCollatorWithPadding is a class in the Hugging Face library) is created to form these batches of the data. We are using the tokenizer we made earlier and a padding strategy 'max_length', which means that the sequences will be padded to the maximum length in the batch. The data collator will create uniform batch sizes, which allow efficient parallel processing in deep learning frameworks, by using Truncation and Padding. These are 2 different ways of achieving uniform batch sizes.\n","It trunctuates the sequences which exceed the maximum length, and pads the sequences which are less than the maximum length. The maximum length is determined by the maximum length of the input sequences after they have been tokenized by the tokenizer. Truncation involves shortening a sequence by removing tokens from the end. While padding involves adding special tokens (usually zeros) to the end of a sequence to make it equal in length to the maximum sequence length within a batch."]},{"cell_type":"markdown","metadata":{},"source":["## Creating Train and Test set\n","\n","We split the tokenized dataset into training and test sets, %70 for training, %30 for testing. Our dataset is now the following:\n","\n","<img src=\"reportImages/datasetSplit.jpg\" style=\"border:solid 1px white;\"/>\n","\n","We define the training set and testing set individually so we can pass them to model later. They are called tok_train_dataset and tok_test_dataset."]},{"cell_type":"markdown","metadata":{},"source":["## Creating an Evaluation Metric\n","\n","We use the library called \"Evaluate\" for evaluating our model. It is a library with a wide range of evaluation tools. There are 3 types of evaluations in it, one of them is \"Metric\" which we will be using. It measures the performance of a model on a given dataset, usually by comparing the model's predictions to some ground truth labels. We will be using an \"Accuracy\" metric from this type. According to the documentation on the Hugging Face website: Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with: Accuracy = (TP + TN) / (TP + TN + FP + FN) Where: TP: True positive TN: True negative FP: False positive FN: False negative.\n","\n","Accuracy is a common evaluation metric used to measure the accuracy of a classification model. So it is suitable for our case in binary classification for toxic and non-toxic comments.\n","\n","We then make a compute_metrics function to be used later in the evaluation of the model. As a parameter, we pass the model's predictions (eval_pred). The function extracts the predicted labels, compares them to the true labels (references), and calculates the accuracy using the loaded accuracy metric. The final accuracy value is then returned. This function encapsulates the logic for computing accuracy during the evaluation phase and will be used in a later step in the model trainer."]},{"cell_type":"markdown","metadata":{},"source":["## Making the model\n","\n","And now to make the model. First, we initialize a sequence classification model (we'll give it the name \"model\") and train it using the Hugging Face Transformers library. The AutoModelForSequenceClassification class is used to load a pre-trained model for our binary sequence classification task. The pre-trained model is the same as the one specified earlier in the tokenization step: distilled version of the BERT base model (DistilBERT). We have 2 labels (binary), 1 for toxic and 0 for not toxic. Additional parameters such as id2label and label2id are provided for label mapping, and output_attentions=True allows the model to output attention weights during training. In NLP, the attention weights are calculated based on the relevance or similarity between the elements and a query or context vector.\n","\n","We then define the training arguments using a TrainingArguments class, we'll give it the name \"training_args\". These arguments include:\n","\n","- output_dir: The directory where the model predictions and checkpoints will be written.\n","- learning_rate: The learning rate of the optimizer, it is set to 2e-5\n","- warmup_ratio: The ratio of steps used for the warmup phase of the learning rate scheduler, it is set to 0.1\n","- lr_scheduler_type: The type of learning rate scheduler to use, it is set to a cosine learning rate scheduler.\n","- fp16: Precision training, it is set to 16-bit.\n","- num_train_epochs: The number of epochs to train the model for, it is set to 2 epochs. We could add more but for now only 2 is enough.\n","- weight_decay: The weight decay to use for the optimizer, it is set to 0.01. \n","\n","Let's elaborate more on some points, specifically the cosine learning rate scheduler, fp16, and the weight decay. This type of scheduler will reduce the learning rate from its initial value to a minimum value over the course of the training, and then increase it again to its initial value by the end of the training. As for fp16, it is set to True which means that the model will use 16-bit precision training instead of 32-bit training. This can help reduce the memory requirements of the model and speed up training. As for weight decay, it is a regularization technique that helps prevent overfitting by adding a penalty term to the loss function. The penalty term is proportional to the square of the magnitude of the weights, which encourages the model to learn smaller weights. This can help prevent the model from fitting the training data too closely and improve its generalization performance on new data. In our case, the penalty term added to the loss function will be proportional to the square of the magnitude of the weights multiplied by 0.01.\n","\n","And now to actually train the model, we will ise the Trainer class. This class is part of the PyTorch Lightning library, which is a lightweight wrapper around PyTorch that provides a high-level interface for training deep learning models. We make an instance of this class with the following arguments (All of the arguments passed were defined in previous steps):\n","\n","- model: The machine learning model to be trained. This is the sequence classification model (model).\n","- args: The training arguments (training_args).\n","- train_dataset: The dataset to be used for training the model (tok_train_dataset).\n","- eval_dataset: The dataset to be used for evaluating the model (tok_test_dataset).\n","- tokenizer: The tokenizer to be used for tokenizing the input data (tokenizer).\n","- data_collator: The data collator to be used for collating the input data (data_collator).\n","- compute_metrics: The function to be used for computing the evaluation metrics (compute_metrics).\n","\n","We finally start training the model using the \"train\" method of the Trainer object. It iterates through epochs, updating the model's weights based on the training dataset and evaluating performance on the validation set.\n","\n","The following is the output when we train the model:"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2023-12-12T14:12:25.733179Z","iopub.status.busy":"2023-12-12T14:12:25.732775Z","iopub.status.idle":"2023-12-12T14:15:04.091552Z","shell.execute_reply":"2023-12-12T14:15:04.090391Z","shell.execute_reply.started":"2023-12-12T14:12:25.733147Z"},"id":"9tN3brjKSfxY","outputId":"1cd24289-485d-4465-aedf-d1aee4a291e3","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20231212_141246-hq9emzmu</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/dina_team_6/huggingface/runs/hq9emzmu' target=\"_blank\">splendid-dawn-4</a></strong> to <a href='https://wandb.ai/dina_team_6/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/dina_team_6/huggingface' target=\"_blank\">https://wandb.ai/dina_team_6/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/dina_team_6/huggingface/runs/hq9emzmu' target=\"_blank\">https://wandb.ai/dina_team_6/huggingface/runs/hq9emzmu</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [350/350 01:39, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=350, training_loss=0.2803288922991071, metrics={'train_runtime': 150.4657, 'train_samples_per_second': 37.218, 'train_steps_per_second': 2.326, 'total_flos': 741817432473600.0, 'train_loss': 0.2803288922991071, 'epoch': 2.0})"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["# Training the model on our data with our specific training arguements\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tok_train_dataset,\n","    eval_dataset=tok_test_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Saving the model\n","\n","We save the traind model along with other training rlated information through the save_model method of the Trainer object. This will be the model we'll use in Streamlit later. The model's name is \"comments_model\".\n","\n","We had also tried saving the model into a pickle file but this only saved the raw model object as is, without any additional information related to training so we didn't use it in the streamlit app."]},{"cell_type":"markdown","metadata":{},"source":["## Testing the model\n","\n","And now to test our model!\n","\n","The AutoTokenizer and AutoModelForSequenceClassification classes from the Hugging Face Transformers library are used to load the tokenizer and model, respectively. After loading the tokenizer and model, a text classification pipeline is created using the loaded model and tokenizer. The TextClassificationPipeline class from the Transformers library is used to create this pipeline.\n","\n","Finally, the pipeline is used to make predictions on two sample texts, “You are beautiful” (which is expected to be a \"not toxic\" comment) and “You are ugly” (which is expected to be a \"toxic\" comment)."]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T14:18:51.637297Z","iopub.status.busy":"2023-12-12T14:18:51.636772Z","iopub.status.idle":"2023-12-12T14:18:52.331914Z","shell.execute_reply":"2023-12-12T14:18:52.330792Z","shell.execute_reply.started":"2023-12-12T14:18:51.637248Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'label': 'Not Toxic', 'score': 0.7726908326148987}]\n","[{'label': 'Toxic', 'score': 0.9823653101921082}]\n"]}],"source":["# Load the pre-trained tokenizer and model\n","model_path = \"/kaggle/working/comments_model\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForSequenceClassification.from_pretrained(model_path)\n"," \n","# Create a text classification pipeline using the loaded model and tokenizer\n","pipeline =  TextClassificationPipeline(model=model, tokenizer=tokenizer)\n","\n","# Make predictions on sample texts and print the results\n","print(pipeline(\"You are beautiful\"))\n","print(pipeline(\"You are ugly\"))"]},{"cell_type":"markdown","metadata":{},"source":["As expected, the model classified the first as not toxic (with an accuracy score of %77) and the second as toxic (with an accuracy score of %98)."]},{"cell_type":"markdown","metadata":{},"source":["## Extra: Include a Streamlit deployment\n","\n","Streamlit app URL: https://toxiccomments.streamlit.app/\n","\n","We made an app where we showcase the EDA we performed and provide a text box for users to enter a comment. Our model then makes a prediction on it and classifies whether it's toxic or not.\n","\n","<img src=\"reportImages/streamlit1.png\" width=\"900\"/>\n","\n","<img src=\"reportImages/streamlit2.png\" width=\"900\"/>"]},{"cell_type":"markdown","metadata":{},"source":["## NLP Classification Toxic-Comments Code\n","\n","The full code can be found in toxic-comments-final.ipynb\n","\n","Streamlit code can be found in NLPStreamlit/streamlit.py"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion\n","\n","To conclude, even though we encountered some problems here and there, overall it was doable. We are proud of our toxic-comment classification model and it's accuracy scores, as well as the streamlit app."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4151559,"sourceId":7182320,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08b81f61b85c47449781b01740371de3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1298db08f0154838bbef3ff6b68514e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30d05b5ca79b437ea0c4b07c12ec81ca","placeholder":"​","style":"IPY_MODEL_9adeccff48bd4171bc4dbfadf7e86371","value":" 159571/159571 [03:56&lt;00:00, 745.58 examples/s]"}},"14a5b890a9cc4ee6999df60d9c53be70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30d05b5ca79b437ea0c4b07c12ec81ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b9e8d8732254002a3c1c5719d418132":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eda5a85740aa4cbe82a4f003d76ddb65","max":159571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_730e5d4b30e44e8a936296240ad87f12","value":159571}},"730e5d4b30e44e8a936296240ad87f12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91460ce95f56438dae180b72003d7b60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9adeccff48bd4171bc4dbfadf7e86371":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9d53553da52458db77a4a32d9009f38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91460ce95f56438dae180b72003d7b60","placeholder":"​","style":"IPY_MODEL_14a5b890a9cc4ee6999df60d9c53be70","value":"Map: 100%"}},"eda5a85740aa4cbe82a4f003d76ddb65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef6c3ec157194946908db760941f9bb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9d53553da52458db77a4a32d9009f38","IPY_MODEL_3b9e8d8732254002a3c1c5719d418132","IPY_MODEL_1298db08f0154838bbef3ff6b68514e5"],"layout":"IPY_MODEL_08b81f61b85c47449781b01740371de3"}}}}},"nbformat":4,"nbformat_minor":4}
