{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import base64\n",
    "from selenium.webdriver.common.by import By\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders(query):\n",
    "    # Create the main \"images\" folder if it does not exis\n",
    "    if not os.path.exists('datasetsingrid'):\n",
    "        os.makedirs('datasetsingrid')\n",
    "\n",
    "    # Create the training set folder within the \"images\" folder if it does not exist\n",
    "    training_folder = os.path.join('datasetsingrid', 'training_set')\n",
    "    if not os.path.exists(training_folder):\n",
    "        os.makedirs(training_folder)\n",
    "\n",
    "    # Create a folder within the training set folder with the query variable as its name if it does not exist\n",
    "    query_training_folder = os.path.join(training_folder, query)\n",
    "    if not os.path.exists(query_training_folder):\n",
    "        os.makedirs(query_training_folder)\n",
    "\n",
    "    # Create the test set folder within the \"images\" folder if it does not exist\n",
    "    test_folder = os.path.join('datasetsingrid', 'test_set')\n",
    "    if not os.path.exists(test_folder):\n",
    "        os.makedirs(test_folder)\n",
    "\n",
    "    # Create a folder within the test set folder with the query variable as its name if it does not exist\n",
    "    query_test_folder = os.path.join(test_folder, query)\n",
    "    if not os.path.exists(query_test_folder):\n",
    "        os.makedirs(query_test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_urls, query, folder):\n",
    "    #Loop through the retrieved urls.\n",
    "    for i, image_url in enumerate(image_urls):\n",
    "        #Fetch the content of the image url\n",
    "        image_data = requests.get(image_url).content\n",
    "        #If random.random is less than 0.8 the images is saved in the training set.\n",
    "        #Otherwise they are saved in the test set, giving an approximate 80/20 split of the images.\n",
    "        #This is not 100% exact.\n",
    "        if random.random() <= 0.8:\n",
    "            target_dir = os.path.join('datasetsingrid','training_set', folder)\n",
    "        else:\n",
    "            target_dir = os.path.join('datasetsingrid','test_set', folder)\n",
    "        #Save the file in the correct folder\n",
    "        with open(os.path.join(target_dir, f\"{query}_{i}.jpg\"), 'wb') as image_file:\n",
    "                image_file.write(image_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images(query):\n",
    "    #Initialize the web driver/Open the browser\n",
    "    driver = webdriver.Chrome()\n",
    "    #The page to open with the query given as parameter\n",
    "    driver.get(f\"https://www.google.com/search?q={query}&tbm=isch\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        accept_all_button = driver.find_element(By.XPATH, \"//button[contains(., 'Accept all')]\")\n",
    "        accept_all_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    #Scroll the page 10 times\n",
    "    for _ in range(10):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    #Initialize BeautifulSoup object    \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    #Close the driver/webbrowser\n",
    "    driver.quit()\n",
    "    #Find all the elements with the img tag and a specified class\n",
    "    img_tags = soup.findAll('img', class_='rg_i Q4LuWd')\n",
    "    #Empty list\n",
    "    img_urls = []\n",
    "    #Loop through the html tags\n",
    "    for img_tag in img_tags:\n",
    "        #Check for the data-src attribute\n",
    "        if img_tag.has_attr('data-src'):\n",
    "            #Extract the data-src value\n",
    "            img_urls.append(img_tag['data-src'])\n",
    "    #Return a list with the collected URLs\n",
    "    return list(img_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    #'maple': ['maple tree leaf'],\n",
    "    #'birch':['birch tree leaf','bjÃ¸rkblad'],\n",
    "    #'oak': ['oak tree leaf', 'oak leaf eikenblad'],\n",
    "    #'fraxinus': ['fraxinus','ash three leaf'],\n",
    "    #'salix_fragilis': ['salix_fragilis', 'crack willow tree leaf', 'brittle willow tree leaf']\n",
    "    'ilex': ['ilex']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in categories.items():\n",
    "    #Create folders with the key as folder name\n",
    "    create_folders(key)\n",
    "    #Loop over the values\n",
    "    for value in values:\n",
    "        #Scrape images with the values as search terms\n",
    "        img_urls = scrape_images(value)\n",
    "        #Download the scraped images\n",
    "        download_images(img_urls,value, key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLStreamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
